{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHs3WPn3fho9",
        "outputId": "33bda835-435b-4a65-a634-d45a9618fd42"
      },
      "id": "tHs3WPn3fho9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK5psndO1GgW",
        "outputId": "023ad47c-4526-4364-d62c-233c0f927726"
      },
      "id": "EK5psndO1GgW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/27.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/27.9 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/27.9 MB\u001b[0m \u001b[31m310.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/27.9 MB\u001b[0m \u001b[31m176.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m26.3/27.9 MB\u001b[0m \u001b[31m326.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m317.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m158.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "csPOMsdw0v4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518b0a6b-602f-4893-cd2a-a725e5e789c9"
      },
      "id": "csPOMsdw0v4_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard) (3.3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard) (12.0.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (6.33.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, tensorboard\n",
            "Successfully installed tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Id5zrPc4zDNF"
      },
      "id": "Id5zrPc4zDNF"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEEGDataset(Dataset):\n",
        "    def __init__(self, sentence_mapping, eeg_path, pad_len=5500, dtype=torch.float32):\n",
        "        self.records = pd.read_csv(sentence_mapping)\n",
        "        self.records = self.records.reset_index(drop=True)\n",
        "        self.eeg_path = Path(eeg_path)\n",
        "        self.pad_len = int(pad_len)\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.records.iloc[idx]\n",
        "        uuid = row[\"UniqueID\"]\n",
        "        sentence = row[\"Content\"]\n",
        "\n",
        "        eeg = self._load_and_pad_eeg(uuid)  # [105, T] -> [105, pad_len]\n",
        "        # Return tensors without batch dimension; DataLoader will stack them\n",
        "        return {\n",
        "            \"uuid\": uuid,\n",
        "            \"sentence\": sentence,        # stays as string; DataLoader will make a list of strings\n",
        "            \"eeg\": eeg                   # torch.FloatTensor [105, pad_len]\n",
        "        }\n",
        "\n",
        "    def _load_and_pad_eeg(self, uuid: str) -> torch.Tensor:\n",
        "        path = self.eeg_path / f\"{uuid}.csv\"\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"EEG file not found: {path}\")\n",
        "\n",
        "        # Expect shape [channels=105, timesteps]; adjust if your CSV is transposed\n",
        "        df = pd.read_csv(path)\n",
        "        arr = df.values.astype(np.float32)\n",
        "\n",
        "        # If CSV is [T, 105] instead of [105, T], transpose:\n",
        "        if arr.shape[0] == 5500 and arr.shape[1] == 105:  # heuristic; change to your rule\n",
        "            arr = arr.T\n",
        "\n",
        "        # Pad or crop to pad_len along time axis (axis=1)\n",
        "        c, t = arr.shape\n",
        "        if c != 105:\n",
        "            raise ValueError(f\"Expected 105 channels, got {c} in {path}\")\n",
        "\n",
        "        if t < self.pad_len:\n",
        "            pad = np.zeros((c, self.pad_len - t), dtype=np.float32)\n",
        "            arr = np.concatenate([arr, pad], axis=1)\n",
        "        elif t > self.pad_len:\n",
        "            arr = arr[:, :self.pad_len]\n",
        "\n",
        "        return torch.from_numpy(arr).to(self.dtype)  # [105, pad_len]\n"
      ],
      "metadata": {
        "id": "S2OItP_NZs9b"
      },
      "id": "S2OItP_NZs9b",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "28a526fa",
      "metadata": {
        "id": "28a526fa"
      },
      "source": [
        "# ENCODER - EEG to CODEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1a20d751",
      "metadata": {
        "id": "1a20d751"
      },
      "outputs": [],
      "source": [
        "class ConvolutionModel(nn.Module):\n",
        "    '''\n",
        "    Input : single sentence EEG raw input of (105 channels, 5500 timestamps)\n",
        "    Output : single sentence of 57 features of 512 dimensions embedding each\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convolutional_model = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=105, kernel_size=10, out_channels=64, stride=3),\n",
        "            nn.Conv1d(in_channels=64,  kernel_size=3,  out_channels=128, stride=2),\n",
        "            nn.Conv1d(in_channels=128, kernel_size=3,  out_channels=256, stride=2),\n",
        "            nn.Conv1d(in_channels=256, kernel_size=3,  out_channels=512, stride=2),\n",
        "            nn.Conv1d(in_channels=512, kernel_size=2,  out_channels=512, stride=2),\n",
        "            nn.Conv1d(in_channels=512, kernel_size=2,  out_channels=512, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape expected: [batch_size, channels, timestamps]\n",
        "        op = self.convolutional_model(x)\n",
        "        # Output shape is [batch_size, d_model, num_tokens] -> transpose to [batch_size, num_tokens, d_model]\n",
        "        return op.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-head scaled dot-product attention using nn.Linear layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_k):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_Q(x)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        output = attn_weights @ V\n",
        "        return output"
      ],
      "metadata": {
        "id": "ka_3SUR2y1q1"
      },
      "id": "ka_3SUR2y1q1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.d_model = d_model\n",
        "        # d_k is the dimension of each head. Must be divisible by d_model\n",
        "        self.d_k = d_model // heads\n",
        "\n",
        "        # Use nn.ModuleList to register attention blocks with PyTorch\n",
        "        self.attentionBlocks = nn.ModuleList([AttentionBlock(d_model, self.d_k) for _ in range(self.heads)])\n",
        "\n",
        "        # Final linear layer to project concatenated heads back to d_model dimension\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = []\n",
        "        for block in self.attentionBlocks:\n",
        "            # Assuming x is [batch_size, num_tokens, d_model]\n",
        "            out_h = block(x) # [batch_size, num_tokens, d_k]\n",
        "            head_outputs.append(out_h)\n",
        "\n",
        "        # Concatenate along the last dimension\n",
        "        total = torch.cat(head_outputs, dim=-1) # [batch_size, num_tokens, d_model]\n",
        "\n",
        "        # Apply final linear layer\n",
        "        output = self.output_linear(total)\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZlM7JOISy71J"
      },
      "id": "ZlM7JOISy71J",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model=512, heads=8, beta=0.2):\n",
        "        super().__init__()\n",
        "        self.conv = ConvolutionModel()\n",
        "        self.mha = MultiHeadAttentionBlock(d_model=d_model, heads=heads)\n",
        "        self.codex = nn.Embedding(num_embeddings=2048, embedding_dim=512)\n",
        "        self.words = self.codex.weight  # codebook\n",
        "        self.beta = beta               # commitment weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, 105, 5500]\n",
        "\n",
        "        returns:\n",
        "            z_q_st: [batch_size, 57, 512]  (quantized, straight-through)\n",
        "            vq_loss: scalar (codebook + commitment terms)\n",
        "            indices: [batch_size, 57]  (chosen code indices)\n",
        "        \"\"\"\n",
        "        # conv_output: [batch_size, 57, 512]\n",
        "        conv_output = self.conv(x)\n",
        "\n",
        "        # attn_output: [batch_size, 57, 512] (this is z_c in VQ-VAE terms)\n",
        "        attn_output = self.mha(conv_output)\n",
        "\n",
        "        # --- vector quantization using your codex / words ---\n",
        "        B, L, D = attn_output.shape  # [B, 57, 512]\n",
        "\n",
        "        codebook = self.words.unsqueeze(0)\n",
        "        #atthention output of size [ batch_size, 57, 512 ] but its' context aware now\n",
        "        distances = torch.cdist(attn_output, codebook) # distances of each of 57 EEG feature with the 2048 words in codex book\n",
        "\n",
        "        # indices of the least-distance codex word for each EEG feature\n",
        "        # indices: [B, 57]\n",
        "        indices = torch.argmin(distances, dim=-1)\n",
        "\n",
        "        z_q = self.words[indices]\n",
        "\n",
        "        # --- VQ codebook + commitment losses ---\n",
        "\n",
        "        # codebook loss: || sg[attn_output] - z_q ||^2  (update codex/words)\n",
        "        codebook_loss = F.mse_loss(z_q, attn_output.detach())\n",
        "\n",
        "        # commitment loss: || attn_output - sg[z_q] ||^2  (update encoder)\n",
        "        commitment_loss = F.mse_loss(attn_output, z_q.detach())\n",
        "\n",
        "        vq_loss = codebook_loss + self.beta * commitment_loss\n",
        "\n",
        "        # straight-through: forward uses z_q, grads go to attn_output\n",
        "        z_q_st = attn_output + (z_q - attn_output).detach()\n",
        "\n",
        "        return z_q_st, vq_loss, indices"
      ],
      "metadata": {
        "id": "F0XBlWuBy3-E"
      },
      "id": "F0XBlWuBy3-E",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2a77a667",
      "metadata": {
        "id": "2a77a667"
      },
      "source": [
        "# DECODER - self reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "265e70ec",
      "metadata": {
        "id": "265e70ec"
      },
      "outputs": [],
      "source": [
        "class DeConvolutionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Input  (to this block): [B, 512, 57]\n",
        "    Output (from this block): [B, 105, 5500]\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=2, stride=2),  # 57 -> 114\n",
        "            nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=2, stride=2),  # 114 -> 228\n",
        "            nn.ConvTranspose1d(in_channels=512, out_channels=256, kernel_size=3, stride=2),  # 228 -> 457\n",
        "            nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=2),  # 457 -> 915\n",
        "            nn.ConvTranspose1d(in_channels=128, out_channels=64,  kernel_size=3, stride=2),  # 915 -> 1831\n",
        "            nn.ConvTranspose1d(in_channels=64,  out_channels=105, kernel_size=10, stride=3), # 1831 -> 5500\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.deconv(x)  # [B, 105, 5500]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model=512, heads=8):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttentionBlock(d_model=d_model, heads=heads)  # keeps [B, 57, 512]\n",
        "        self.deconv = DeConvolutionModel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 57, 512]\n",
        "        attn_out = self.mha(x)                 # [B, 57, 512]\n",
        "        attn_out = attn_out.permute(0, 2, 1)   # -> [B, 512, 57]  (channels = 512, length = 57)\n",
        "        out = self.deconv(attn_out)            # -> [B, 105, 5500]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2bef323",
      "metadata": {
        "id": "e2bef323"
      },
      "source": [
        "# WORD2VEC - converting sentences to embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0e75671b",
      "metadata": {
        "id": "0e75671b"
      },
      "outputs": [],
      "source": [
        "class Word2VecModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.w2v = api.load('word2vec-google-news-300')\n",
        "        self.linear = nn.Linear(300, 512)\n",
        "        self.unk_vector = torch.zeros(300)  # vector for unknown words\n",
        "\n",
        "    def forward(self, sentence_tokens):\n",
        "        \"\"\"\n",
        "        sentence_tokens: list of tokens (strings)\n",
        "        returns: [num_words, 512] tensor\n",
        "        \"\"\"\n",
        "        device = self.linear.weight.device\n",
        "\n",
        "        vectors = []\n",
        "        for word in sentence_tokens:\n",
        "            if word in self.w2v:\n",
        "                vec = torch.tensor(self.w2v[word], dtype=torch.float32, device=device)\n",
        "            else:\n",
        "                vec = self.unk_vector.to(device)\n",
        "            vectors.append(vec)\n",
        "\n",
        "        emb = torch.stack(vectors)              # [num_words, 300]\n",
        "        enh_emb = self.linear(emb)              # [num_words, 512]\n",
        "        # KEY STEP: Interpolate text to match EEG length\n",
        "        z_t_interpolated = F.interpolate(\n",
        "            enh_emb.transpose(0, 1).unsqueeze(0),  # [1, 512, num_words]\n",
        "            size=57,                            # Match EEG length\n",
        "            mode='linear',\n",
        "            align_corners=True\n",
        "        ).squeeze(0).transpose(0, 1)  # [57, 512]\n",
        "\n",
        "        return z_t_interpolated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "mpKq_FyP2r5j"
      },
      "id": "mpKq_FyP2r5j"
    },
    {
      "cell_type": "code",
      "source": [
        "def nt_xent_loss(out, target, temperature=0.07):\n",
        "  zq = F.normalize(out,dim=1)\n",
        "  zt = F.normalize(target,dim=1)\n",
        "\n",
        "  logits = zq @ zt.T / temperature #Build similarity matrix. Smaller τ → make similarities more “peaked” → harder classification → stronger gradients.\n",
        "\n",
        "  labels = torch.arange(logits.shape[0], device=logits.device)\n",
        "  #For row 0: the correct target index is 0 → wants softmax to choose column 0 and so on for all the rows\n",
        "\n",
        "  loss = F.cross_entropy(logits, labels)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "voQjWeDCykW0"
      },
      "id": "voQjWeDCykW0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, encoder, decoder, w2v, optimizer, loss, path):\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"encoder_state\": encoder.state_dict(),\n",
        "        \"decoder_state\": decoder.state_dict(),\n",
        "        \"w2v_state\": w2v.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"loss\": loss,\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "    print(f\"✔ Saved checkpoint at epoch {epoch} to {path}\")"
      ],
      "metadata": {
        "id": "JMCgEDYho5mI"
      },
      "id": "JMCgEDYho5mI",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(path, encoder, decoder, w2v, optimizer=None):\n",
        "    checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "\n",
        "    encoder.load_state_dict(checkpoint[\"encoder_state\"])\n",
        "    decoder.load_state_dict(checkpoint[\"decoder_state\"])\n",
        "    w2v.load_state_dict(checkpoint[\"w2v_state\"])\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "\n",
        "    print(f\"✔ Loaded checkpoint from {path}, epoch {checkpoint['epoch']}\")\n",
        "    return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n"
      ],
      "metadata": {
        "id": "KgAbRUWno_Qo"
      },
      "id": "KgAbRUWno_Qo",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i,data in enumerate(dataloader):\n",
        "      \"\"\"\n",
        "      every data is {\n",
        "            \"uuid\": uuid,\n",
        "            \"sentence\": sentence,        # stays as string; DataLoader will make a list of strings\n",
        "            \"eeg\": eeg                   # torch.FloatTensor [105, pad_len]\n",
        "        }\n",
        "      \"\"\"\n",
        "      sentence = data[\"sentence\"]\n",
        "      eeg = data[\"eeg\"].to(device)\n",
        "\n",
        "      z_q, vq_loss, indices = encoder(eeg)  # z_q: [B, 57, 512]\n",
        "\n",
        "      predictions = decoder(z_q)            # same as before\n",
        "      reconstruction_loss = F.mse_loss(predictions, eeg)\n",
        "\n",
        "      # sentence is a list of B strings\n",
        "      # Convert each sentence to tokens; here I just use .split() as a simple tokenizer\n",
        "      text_embeddings_list = [w2v(s.split()) for s in sentence]  # each: [57, 512]\n",
        "\n",
        "      # Stack into [B, 57, 512]\n",
        "      text_embeddings = torch.stack(text_embeddings_list, dim=0)  # [B, 57, 512]\n",
        "\n",
        "\n",
        "      # if text_embeddings is [B, L_text, 512], pool to [B, 512]\n",
        "      z_q_pooled = z_q.mean(dim=1)                # [B, 512]\n",
        "      text_pooled = text_embeddings.mean(dim=1)   # [B, 512]\n",
        "\n",
        "      contrastive_loss = nt_xent_loss(z_q_pooled, text_pooled)\n",
        "\n",
        "      L_wave = reconstruction_loss + vq_loss\n",
        "      loss = L_wave + contrastive_loss            # or + alpha * contrastive_loss\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if writer is not None and (i % 50 == 0):\n",
        "          writer.add_scalar(\"train/loss_total\", loss.item(), epoch_index * len(dataloader) + i)\n",
        "          writer.add_scalar(\"train/loss_recon\",  reconstruction_loss.item(), epoch_index * len(dataloader) + i)\n",
        "          writer.add_scalar(\"train/loss_contrast\", contrastive_loss.item(), epoch_index * len(dataloader) + i)\n",
        "\n",
        "    return running_loss / max(1, len(dataloader))"
      ],
      "metadata": {
        "id": "kvRwFxSI2rfe"
      },
      "id": "kvRwFxSI2rfe",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_mapping = '/content/drive/MyDrive/EEG_dataset/dataset/sentence_mapping.csv'\n",
        "eeg_path = '/content/drive/MyDrive/EEG_dataset/dataset'\n",
        "writer = SummaryWriter('runs/eeg_training_experiment_1')\n",
        "\n",
        "# hyperparams\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LR = 1e-3\n",
        "ALPHA = 1.0   # weight for contrastive loss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# models\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "w2v = Word2VecModel().to(device)\n",
        "\n",
        "# dataset & dataloader\n",
        "dataset = CustomEEGDataset(\n",
        "    sentence_mapping=sentence_mapping,\n",
        "    eeg_path=eeg_path,\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) +\n",
        "    list(decoder.parameters()) +\n",
        "    list(w2v.parameters()),\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "\n",
        "# OPTIONAL: resume from an existing checkpoint\n",
        "resume = True\n",
        "resume_path = \"/content/drive/MyDrive/EEG_dataset/checkpoints/epoch_26.pt\"  # example\n",
        "\n",
        "start_epoch = 0\n",
        "if resume:\n",
        "    start_epoch, last_loss = load_checkpoint(\n",
        "        path=resume_path,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        w2v=w2v,\n",
        "        optimizer=optimizer,\n",
        "    )\n",
        "    print(f\"Resuming from epoch {start_epoch} with loss = {last_loss:.4f}\")\n",
        "\n",
        "for epoch in range(start_epoch + 1, EPOCHS):\n",
        "    avg_loss = train_one_epoch(epoch_index=epoch, tb_writer=writer)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{EPOCHS}] - loss: {avg_loss:.4f}\")\n",
        "    save_checkpoint(\n",
        "        epoch=epoch,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        w2v=w2v,\n",
        "        optimizer=optimizer,\n",
        "        loss=avg_loss,\n",
        "        path=f\"/content/drive/MyDrive/EEG_dataset/checkpoints/epoch_{epoch}.pt\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "1QbwRojymQc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b37301d-2471-4bbb-8866-3a15c8a7e8fc"
      },
      "id": "1QbwRojymQc4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "✔ Loaded checkpoint from /content/drive/MyDrive/EEG_dataset/checkpoints/epoch_26.pt, epoch 26\n",
            "Resuming from epoch 26 with loss = 23462915.9986\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V6E1",
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}